{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거 전 학습 데이터셋: 64770\n",
      "중복 제거 전 테스트 데이터셋: 16193\n",
      "중복 제거 후 학습 데이터셋: 59709\n",
      "중복 제거 후 테스트 데이터셋: 15350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, TrainerCallback\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# gpu 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "df = pd.read_csv(\"./Data/emotion3.tsv\", sep=\"\\t\")\n",
    "null_idx = df[df[\"document\"].isnull()].index\n",
    "df.loc[null_idx]\n",
    "train_data = df.sample(frac=0.8, random_state=42)\n",
    "test_data = df.drop(train_data.index)\n",
    "print(\"중복 제거 전 학습 데이터셋: {}\".format(len(train_data)))\n",
    "print(\"중복 제거 전 테스트 데이터셋: {}\".format(len(test_data)))\n",
    "\n",
    "# 중복 데이터 제거\n",
    "train_data = train_data.drop_duplicates([\"document\"])\n",
    "test_data = test_data.drop_duplicates([\"document\"])\n",
    "\n",
    "# 데이터셋 갯수 확인\n",
    "print(\"중복 제거 후 학습 데이터셋: {}\".format(len(train_data)))\n",
    "print(\"중복 제거 후 테스트 데이터셋: {}\".format(len(test_data)))\n",
    "MODEL_NAME = \"beomi/KcELECTRA-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tokenizer_train_sentences = tokenizer(\n",
    "    list(train_data['document']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=128,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True,\n",
    ")\n",
    "tokenizer_test_sentences = tokenizer(\n",
    "    list(test_data['document']),\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=128,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    add_special_tokens=True,\n",
    ")\n",
    "\n",
    "class CurseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_label = train_data['label'].values\n",
    "test_label = test_data['label'].values\n",
    "\n",
    "train_dataset = CurseDataset(tokenizer_train_sentences, train_label)\n",
    "test_dataset = CurseDataset(tokenizer_test_sentences, test_label)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=7).to(device)\n",
    "\n",
    "traning_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }\n",
    "\n",
    "class LogCallback(TrainerCallback):\n",
    "    def on_train_epoch_end(self, args, state, control, **kwargs):\n",
    "        train_loss_values.append(state.log_history[-1][\"train_loss\"])\n",
    "        train_acc_values.append(state.log_history[-1][\"train_accuracy\"])\n",
    "        val_loss_values.append(state.log_history[-1][\"eval_loss\"])\n",
    "        val_acc_values.append(state.log_history[-1][\"eval_accuracy\"])\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=traning_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[LogCallback()],\n",
    ")\n",
    "\n",
    "# 훈련 손실 및 정확도 그래프 그리기\n",
    "train_loss_values = []\n",
    "train_acc_values = []\n",
    "val_loss_values = []\n",
    "val_acc_values = []\n",
    "\n",
    "def log_callback(step, logs):\n",
    "    train_loss_values.append(logs.get(\"train_loss\"))\n",
    "    train_acc_values.append(logs.get(\"train_accuracy\"))\n",
    "    val_loss_values.append(logs.get(\"eval_loss\"))\n",
    "    val_acc_values.append(logs.get(\"eval_accuracy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d117e4646d49a9b918ccd49134f1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6031, 'grad_norm': 7.632872581481934, 'learning_rate': 5e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0331, 'grad_norm': 7.643521785736084, 'learning_rate': 4.862334801762114e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8893, 'grad_norm': 8.588146209716797, 'learning_rate': 4.7246696035242295e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8015, 'grad_norm': 11.804886817932129, 'learning_rate': 4.5870044052863435e-05, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6642, 'grad_norm': 5.024416446685791, 'learning_rate': 4.449339207048459e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6644, 'grad_norm': 7.93439245223999, 'learning_rate': 4.311674008810573e-05, 'epoch': 1.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6558, 'grad_norm': 4.738731861114502, 'learning_rate': 4.1740088105726874e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5432, 'grad_norm': 9.378602027893066, 'learning_rate': 4.036343612334802e-05, 'epoch': 2.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4453, 'grad_norm': 9.513084411621094, 'learning_rate': 3.898678414096916e-05, 'epoch': 2.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.459, 'grad_norm': 7.3465070724487305, 'learning_rate': 3.761013215859031e-05, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.468, 'grad_norm': 5.7838616371154785, 'learning_rate': 3.623348017621145e-05, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.319, 'grad_norm': 11.970169067382812, 'learning_rate': 3.4856828193832605e-05, 'epoch': 3.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.309, 'grad_norm': 5.846876621246338, 'learning_rate': 3.3480176211453745e-05, 'epoch': 3.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3006, 'grad_norm': 9.576818466186523, 'learning_rate': 3.210352422907489e-05, 'epoch': 3.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2965, 'grad_norm': 5.654765605926514, 'learning_rate': 3.072687224669604e-05, 'epoch': 4.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1867, 'grad_norm': 9.607742309570312, 'learning_rate': 2.935022026431718e-05, 'epoch': 4.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1906, 'grad_norm': 11.23072338104248, 'learning_rate': 2.7973568281938327e-05, 'epoch': 4.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1975, 'grad_norm': 12.595696449279785, 'learning_rate': 2.659691629955947e-05, 'epoch': 4.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.18, 'grad_norm': 7.982532024383545, 'learning_rate': 2.522026431718062e-05, 'epoch': 5.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1248, 'grad_norm': 6.26862907409668, 'learning_rate': 2.3843612334801762e-05, 'epoch': 5.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1321, 'grad_norm': 0.18233928084373474, 'learning_rate': 2.246696035242291e-05, 'epoch': 5.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.133, 'grad_norm': 4.263833045959473, 'learning_rate': 2.1090308370044055e-05, 'epoch': 5.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1035, 'grad_norm': 6.154706001281738, 'learning_rate': 1.97136563876652e-05, 'epoch': 6.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0866, 'grad_norm': 15.848109245300293, 'learning_rate': 1.8337004405286344e-05, 'epoch': 6.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0925, 'grad_norm': 2.198878526687622, 'learning_rate': 1.696035242290749e-05, 'epoch': 6.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0962, 'grad_norm': 0.05520634353160858, 'learning_rate': 1.5583700440528634e-05, 'epoch': 6.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0614, 'grad_norm': 0.08202322572469711, 'learning_rate': 1.420704845814978e-05, 'epoch': 7.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0694, 'grad_norm': 5.679375648498535, 'learning_rate': 1.2830396475770926e-05, 'epoch': 7.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0646, 'grad_norm': 11.487119674682617, 'learning_rate': 1.1453744493392071e-05, 'epoch': 7.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0607, 'grad_norm': 0.3119066655635834, 'learning_rate': 1.0077092511013217e-05, 'epoch': 8.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0415, 'grad_norm': 0.07669370621442795, 'learning_rate': 8.700440528634362e-06, 'epoch': 8.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0477, 'grad_norm': 0.3647659718990326, 'learning_rate': 7.323788546255507e-06, 'epoch': 8.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0443, 'grad_norm': 0.1471635103225708, 'learning_rate': 5.947136563876652e-06, 'epoch': 8.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0397, 'grad_norm': 1.858541488647461, 'learning_rate': 4.5704845814977974e-06, 'epoch': 9.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0333, 'grad_norm': 23.360563278198242, 'learning_rate': 3.193832599118943e-06, 'epoch': 9.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.03, 'grad_norm': 0.007693660445511341, 'learning_rate': 1.8171806167400882e-06, 'epoch': 9.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0281, 'grad_norm': 0.3068910837173462, 'learning_rate': 4.4052863436123357e-07, 'epoch': 9.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhy97\\AppData\\Local\\Temp\\ipykernel_19884\\1380925816.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6030.799, 'train_samples_per_second': 99.007, 'train_steps_per_second': 3.094, 'train_loss': 0.3083378611909104, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18660, training_loss=0.3083378611909104, metrics={'train_runtime': 6030.799, 'train_samples_per_second': 99.007, 'train_steps_per_second': 3.094, 'total_flos': 3.9277008194112e+16, 'train_loss': 0.3083378611909104, 'epoch': 10.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss_values, label=\"Train Loss\")\n",
    "plt.plot(val_loss_values, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_acc_values, label=\"Train Accuracy\")\n",
    "plt.plot(val_acc_values, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_model\\\\tokenizer_config.json',\n",
       " './saved_model\\\\special_tokens_map.json',\n",
       " './saved_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델과 토크나이저 저장\n",
    "model.save_pretrained(\"./saved_model\")\n",
    "tokenizer.save_pretrained(\"./saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 1, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 5, Predicted Label: 3\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 1, Predicted Label: 1\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 3, Predicted Label: 3\n",
      "True Label: 6, Predicted Label: 6\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 4, Predicted Label: 4\n",
      "True Label: 5, Predicted Label: 5\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 0, Predicted Label: 0\n",
      "True Label: 2, Predicted Label: 2\n",
      "True Label: 6, Predicted Label: 6\n",
      "정확도: 99.70%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# gpu 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 테스트 데이터로 정확도 검증\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./saved_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./saved_model\")\n",
    "model.to(device)\n",
    "\n",
    "def sentence_predict(sent):\n",
    "    model.eval()\n",
    "    tokenized_sent = tokenizer(\n",
    "        [sent],\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=128,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    tokenized_sent = {k: v.to(device) for k, v in tokenized_sent.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=tokenized_sent[\"input_ids\"],\n",
    "            attention_mask=tokenized_sent[\"attention_mask\"],\n",
    "        )\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu()\n",
    "    result = logits.argmax(-1).numpy()[0]\n",
    "    return result\n",
    "\n",
    "data = pd.read_csv(\"Data/emotion3.csv\")\n",
    "sampled_data = data.sample(n=1000)\n",
    "\n",
    "correct_predictions = 0\n",
    "\n",
    "for index, row in sampled_data.iterrows():\n",
    "    sentence = row['발화문']\n",
    "    true_label = row['상황']\n",
    "    predicted_label = sentence_predict(sentence)\n",
    "    print(f\"True Label: {true_label}, Predicted Label: {predicted_label}\")\n",
    "    if true_label == predicted_label:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / len(sampled_data)\n",
    "print(f\"정확도: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
